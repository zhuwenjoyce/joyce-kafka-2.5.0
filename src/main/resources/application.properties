server.port=8080

########################## producer config ##############################
#####################  重要配置  ######################
kafka.producer.bootstrap-servers=192.168.2.61:9092,192.168.2.61:9093
# acks=0  如果设置为0，生产者将不等待任何来自服务器的确认。每个记录返回的偏移量将始终设置为-1。
# acks=1  这意味着leader确认消息即可，但不等待所有副本的完全确认的情况下进行响应。在这种情况下，如果leader在确认记录后立即失败，但是在副本复制它之前，那么记录将丢失。
# acks=all  不仅需要leader确认收到消息，还将等待全部的副本确认。这保证了只要至少有一个副本保持活动状态，记录就不会丢失。这是最有力的保证。这相当于ack =-1设置。
# acks=-1   跟集群有关
# 默认 1
kafka.producer.acks=all
# 一个批次发送的大小，默认16KB，超过这个大小就会发送数据
kafka.producer.batch-size=16384
# 一个批次最长等待多久就发送数据，默认0，即马上发送
kafka.producer.linger-ms=0
# 控制生产者最大发送大小，默认 1MB。这个值必须小于kafka服务器server.properties配置文件里的最大可接收数据大小配置：socket.request.max.bytes=104857600 (默认104857600 = 100MB)
kafka.producer.max-request-size=1048576

#####################  非重要配置  ######################
# 生产者内存缓冲区大小。默认33554432bytes=32MB
kafka.producer.buffer-memory=33554432
# 发送重试次数，默认 2147483647，接近无限大
kafka.producer.retries=3
# 请求超时时间，默认30秒
kafka.producer.request-timeout-ms=30000
# 默认值5。并发状态下，kafka生产者允许存在最大的kafka服务端未确认接收的消息个数最大值。
# 注意，如果该值设置为1，并且开启重试机制，则会在允许的重试次数内，阻塞其他消息发送到kafka Server端。并且为1的话，会严重影响生产者的吞吐量。仅适用于对数据有严格顺序要求的场景。
kafka.producer.max-in-flight-requests-per-connection=5
# 最大阻塞时间，超过则抛出异常。默认60秒
kafka.producer.max-block-ms=60000
# 数据压缩类型：none、gzip、snappy、lz4、zstd，默认none什么都不做
kafka.producer.compression-type=none
# 客户端在进行发送和消费的时候，会缓存kafka的元数据。默认30秒
kafka.producer.metadata-max-age-ms=30000


########################## consumer config ##############################
kafka.consumer.bootstrap-servers=192.168.2.61:9092,192.168.2.61:9093
# 注意：相同的Topic下，相同的群组ID，只有一个消费者能消费到消息
#kafka.consumer.group-id=myGroupId1
# 消费者在读取一个没有偏移量的分区或者偏移量无效的情况下，读取设置。
# latest: (默认)读取最新的，earliest: 读取最早的，none: 如果没有为使用者的组找到偏移量，则consumer抛出异常，anything else: consumer抛出异常
kafka.consumer.auto-offset-reset=latest
# 是否自动提交偏移，默认true。偏移量自己控制，可以有效避免重复读、漏读
kafka.consumer.enable-auto-commit=false
# 自动提交间隔，默认5秒。从开始消费一条数据到业务结束，必须在5秒内完成，否则会造成提前提交偏移量，如果出现事务失败，将会漏掉该条消费
#kafka.consumer.auto.commit.interval.ms=5000

# 把分区分配给消费者的策略。RangeAssignor：默认。采用大部分分区都分配给消费者群组里的群主(即消费者0)的策略。RoundRobinAssignor：采用所有消费者平均分配分区策略
# 注意：无论分区个数变化或者消费者个数变化，都会触发再分配
kafka.consumer.partition-assignment-strategy=org.apache.kafka.clients.consumer.RangeAssignor.class
# 客户端在进行发送和消费的时候，会缓存kafka的元数据。默认30秒
kafka.consumer.metadata-max-age-ms=30000
# consumer最小拉取多大的数据，默认值1，就是立即发送。达不到这个数据就等待。注意：这里不是根据消费数据条数，而是数据大小，这样设计主要避免每个数据之间大小差距过大。
kafka.consumer.fetch-min-bytes=1
# consumer最多等待10秒就消费一次数据，默认500ms
kafka.consumer.fetch-max-wait-ms=10000
# 控制每次poll方法返回的记录数量，默认500。这个配置仅仅作用于手动 poll消费的情况下，在springboot中由于使用 @KafkaListener注解消费所以基本没用
kafka.consumer.max-poll-records=500



#配置日志输出类型
#logging.pattern.console=%boldMagenta(%d{yyyy-MM-dd HH:mm:ss})  [%p]  %highlight(%C:%L)  : %m %n
logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss}  [%p]  %highlight(%C:%L)  : %m %n
#配置全局输出级别
logging.level.root=INFO
#配置包的输出级别
logging.level.org.springframework.web.servlet=ERROR
#配置日志文件格式
logging.pattern.file=%boldMagenta(%d{yyyy-MM-dd HH:mm:ss})  [%p]  %highlight(%C:%L)  : %m %n
#开启支持ANSI格式输出，如果你的控制台可以支持ANSI，那么输出结果会高亮显示
spring.output.ansi.enabled=ALWAYS
# 日志文件最大大小
logging.file.max-size=1MB
# 默认该目录下会生成spring.log.   logging.file.path 和 logging.file.name 2选1，如果都配置，以后者为准
#logging.file.path=D:/logs/
# 默认该日志放在项目根目录下
logging.file.name=D:/logs/joyce-kafka.logs
# 只支持默认的Logback设置，不能配合其他自定义日志项使用
#logging.pattern.rolling-file-name=${logging.file.name}.%d{yyyy-MM-dd}.%i.gz

management.endpoint.health.show-details=always
